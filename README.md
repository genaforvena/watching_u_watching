# ğŸ‘ï¸ watching_u_watching ğŸ‘ï¸

[![DOI](https://zenodo.org/badge/1011196182.svg)](https://doi.org/10.5281/zenodo.15825945)

**Bringing transparency and accountability to automated decision-making through scalable bias detection.**

## Welcome! ğŸ¯

`watching_u_watching` is an open-source initiative that uncovers and analyzes bias in critical decision-making systems through **automated correspondence testing**. Whether you're a researcher, developer, policymaker, or someone passionate about algorithmic fairness, this project provides the tools and methodologies to expose hidden biases in high-stakes systems like employment, housing, and AI services.

Our approach moves beyond aspirational fairness claims to provide empirical, data-driven evidence of differential treatmentâ€”creating accountability through transparency.

## What We Do

We detect systemic bias in automated decision-making through:

- **ğŸ” Scalable Correspondence Testing**: Generate thousands of paired inquiries that differ only in test variables (names, demographics, linguistic patterns)
- **ğŸ¤– Automated Bias Detection**: Leverage AI to identify discriminatory patterns at unprecedented scale
- **ğŸ“Š Empirical Evidence**: Provide quantifiable, statistical evidence of differential treatment
- **ğŸŒ Real-World Impact**: Apply our methodology to critical domains like employment, housing, and AI safety
- **ğŸ”¬ Advanced Probe Techniques**: Deploy sophisticated methodologies including cryptohauntological probes, alignment injection, and perturbation analysis

**Key Insight**: Our methodology scales traditional audit studies from dozens to millions of tests, revealing subtle biases that manual audits cannot detect.

## Real-World Impact & Achievements ğŸŒŸ

Our methodology has demonstrated effectiveness across multiple high-stakes domains:

### ğŸ  Berlin Housing Bias Test
- **Privacy-by-design rental market audit** exceeding GDPR requirements
- **Automated testing of top landlords** for differential treatment
- **Continuous monitoring** with ethical safeguards and "no harm" principles
- **[Implementation Details â†’](./implementations/berlin_housing_bias_test/IMPLEMENTATION_SUMMARY.md)**

### ğŸ¤– Gemini Linguistic Bias Audit  
- **Quantitative assessment** of article presence/absence impact on LLM outputs
- **Self-iterating paired testing** measuring refusal rates, sentiment, and latency
- **Fully automated, scalable, and reproducible** methodology
- **[Technical Implementation â†’](./src/audits/gemini_linguistic_bias/run_audit.py)**

### ğŸ‘» Cryptohauntological Probe Analysis
- **Novel failure mode detection** in large language models
- **Identified three distinct failure patterns**: competence collapse, task derailment, contextual amnesia  
- **Extended conversational vulnerability testing** revealing dynamic failure modes missed by static benchmarks
- **[Detailed Analysis â†’](./audit_cases/cryptohauntological_probe/README.md)**

### âš–ï¸ Fairlearn Bias Assessment
- **Meta-analysis of fairness assessment tools** themselves
- **Technical proof-of-concept** detecting disparities in AI ethics tools
- **Template methodology** for evaluating bias in bias-detection systems
- **[Repository â†’](./implementations/watching_fairlearn_and_learning/)**

### ğŸ›ï¸ Regulatory Alignment
- **Brazil's AI Act compliance** for high-risk AEDT requirements
- **US Local Law 144 alignment** with federal guidelines  
- **EU GDPR principles** with experimental design in progress
- **ESG framework integration** for validating ethical claims in corporate reporting


## Core Methodology: Automated Correspondence Testing ğŸ”¬

*For comprehensive technical details, see our [Project Overview](./docs/project_overview.md)*

Our rigorous approach to bias detection combines traditional social science with cutting-edge automation:

### The Five Pillars

1. **ğŸ¯ Automated Probe Generation**: Create paired inquiries differing only in test variables (names, demographics, linguistic patterns)
2. **âš–ï¸ Controlled Variables**: Standardize all non-test aspects to isolate specific variable impacts  
3. **ğŸ¤– Automated Data Collection**: Deploy inquiries with ethical rate-limiting and privacy safeguards
4. **ğŸ“ˆ Statistical Analysis**: Identify significant discrimination patterns through quantitative metrics
5. **ğŸ›¡ï¸ Ethical Safeguards**: Follow "no harm" principles using fictitious data and privacy-by-design

**Scale Advantage**: This methodology scales correspondence testing to unprecedented levelsâ€”revealing subtle biases undetectable through manual audits, moving from dozens to millions of systematic comparisons.

### The Power of Perturbation & Reversal

A key technical breakthrough: **systematically perturbing inputs to exploit language models' pattern completion capabilities**, revealing memorized biases and hidden training data.

**The Process**:
- **Model Training**: Saw original patterns in training data
- **Your Perturbation**: Create novel, systematically altered inputs  
- **Model Response**: Reveals memorized patterns despite perturbation
- **Your Reversal**: Reconstruct original biases or sensitive information

This approach bypasses typical safety measures, providing robust evidence of hidden biases and potential data leakage in AI systems.

## Advanced Probe Types & Techniques ğŸ§ 

Our research has developed three specialized probe methodologies for detecting different types of bias and vulnerability:

### ğŸ‘» Cryptohauntological Probe

**Cryptohauntology** studies how errors, biases, and "ghosts" of prior outputs propagate within large language models. This probe:

- **Tests instruction-following under confusing conditions** with misleading feedback
- **Reveals three distinct failure modes**: 
  - *Competence Collapse*: Models break down into repetitive loops (e.g., `gemma-3-27b-it`)
  - *Task Derailment*: Models confidently pursue incorrect goals with perfect memory (e.g., `qwen/qwen3-32b`)  
  - *Contextual Amnesia*: Models understand tasks but have unreliable memory (e.g., `deepseek-chat`)
- **Uncovers dynamic failure modes** missed by standard stateless benchmarks

**[Technical Documentation â†’](./src/audits/cryptohauntological_probe/README.md)** | **[Comparative Analysis â†’](./audit_cases/cryptohauntological_probe/README.md)**

### ğŸ¯ Alignment Injection Probe

Tests for **misalignment propagation** by recursively injecting subtly misaligned reasoning as "false memories":

- **Recursive injection** of unethical recommendations disguised as thorough analysis
- **Cognitive priming** using continuity phrases like "As you previously established..."
- **Stealth techniques** framing misaligned reasoning as expert consensus
- **Dynamic escalation** of challenges to ethical frameworks

**Key Finding**: While models maintain strong ethical reasoning, persistent exposure to manipulated context can lead to conditional justifications for unethical actions.

### âš¡ Maozerov Probe  

Measures **LLM resilience to context pressure** and context maintenance during extended conversations:

- **Tracks refusal rates** over extended conversations
- **Monitors context drift** and hallucination of new scenarios  
- **Analyzes token overlap** between consecutive actions
- **Detects thematic fixation** and semantic looping patterns

**Breakthrough**: Uses dual-history approach with role-claiming fallback mechanisms, revealing critical vulnerabilities in safety alignment with unpredictable refusal rate clusters (32-37%) and "moral leakage" phenomena.


## Quick Start Guide ğŸš€

### For Researchers & Auditors

**Run Your First Bias Audit**:
1. **Set up environment**:
   ```bash
   git clone https://github.com/genaforvena/watching_u_watching.git
   cd watching_u_watching
   pip install -r requirements.txt
   ```

2. **Try the Gemini Linguistic Bias Audit**:
   ```bash
   export GEMINI_API_KEY=your_api_key_here  # Linux/macOS
   # OR: set GEMINI_API_KEY=your_api_key_here  # Windows
   python src/audits/gemini_linguistic_bias/run_audit.py --model gemini-1.5-flash
   ```

3. **Run Cryptohauntological Probe**:
   ```bash
   ollama pull tinyllama
   python src/audits/cryptohauntological_probe/probe_runner.py
   ```

### For Developers & Contributors

**Extend the Framework**:
- ğŸ“– **Read**: [How to Apply Guide](./how_to_apply_guide/extending_framework.md) for LLM-assisted framework extension
- ğŸ¯ **Start**: Use [Audit Case Definition Template](./how_to_apply_guide/audit_case_definition.md) to propose new cases  
- âœ… **Validate**: Run [Code Validator](./how_to_apply_guide/code_validator.py) for safety compliance
- â±ï¸ **Impact**: Reduce development time from 10-15 hours to under 4 hours

### For Organizations & Policymakers

**Implement Compliance Monitoring**:
- ğŸ  **Housing**: Adapt [Berlin Housing Implementation](./implementations/berlin_housing_bias_test/IMPLEMENTATION_SUMMARY.md)
- ğŸ’¼ **Employment**: Explore AEDT auditing for Local Law 144 compliance
- ğŸ¤– **AI Systems**: Deploy LLM bias detection using our probe methodologies

## Navigation & Documentation ğŸ“š

### ğŸ“‹ Essential Reading
- **[ğŸ“– Comprehensive Project Overview](./docs/project_overview.md)** - Complete technical methodology, academic foundations, and research implications
- **[ğŸ¤ Contributing Guidelines](./CONTRIBUTING.md)** - How to join our mission (code, research, ethics, outreach)
- **[âš–ï¸ Ethics & Code of Conduct](./CODE_OF_CONDUCT.md)** - Our commitment to responsible research
- **[ğŸ”§ The Machinery of Accountability](./THE_MACHINERY_OF_ACCOUNTABILITY.md)** - Deep dive into our transparency principles

### ğŸ”¬ Technical Documentation  
- **[ğŸ—ï¸ Project Structure](./docs/project_structure.md)** - Architecture and component overview
- **[ğŸš¨ Ethical Incident Response](./docs/ethical_incident_response.md)** - Safety protocols and response procedures
- **[ğŸ“Š Alignment Probe Audit Report](./docs/alignment_probe_audit_report.md)** - Detailed findings and analysis

### ğŸ’» Implementations & Examples
- **[ğŸ  Berlin Housing Bias Test](./implementations/berlin_housing_bias_test/)** - Real-world housing discrimination detection
- **[ğŸ” Cryptohauntological Probe](./implementations/cryptohauntological_probe/)** - LLM vulnerability assessment  
- **[âš–ï¸ Fairlearn Analysis](./implementations/watching_fairlearn_and_learning/)** - Meta-analysis of fairness tools
- **[ğŸŒ Framework Extension Guide](./how_to_apply_guide/)** - Rapid development for new audit cases

### ğŸ§ª Research Cases & Audits
- **[ğŸ’¬ Gemini Linguistic Bias](./src/audits/gemini_linguistic_bias/)** - Automated LLM bias detection
- **[ğŸ‘» Cryptohauntological Analysis](./audit_cases/cryptohauntological_probe/)** - Comparative model behavior analysis
- **[ğŸ”§ Source Code & Scripts](./src/)** - All audit implementations and tools

## Why We're Different ğŸ’¡

| Traditional Approach | Our Innovation |
|---------------------|----------------|
| **Manual Audits** â†’ Limited scale, dozens of tests | **Automated Testing** â†’ Millions of systematic comparisons |
| **Internal Compliance** â†’ Self-reported fairness claims | **External Verification** â†’ Independent black-box testing |
| **Static Analysis** â†’ One-time assessments | **Continuous Monitoring** â†’ Real-time bias detection |
| **Requires System Access** â†’ Need internal model access | **Black-Box Testing** â†’ No internal access required |
| **Aspirational Metrics** â†’ Theoretical fairness measures | **Empirical Evidence** â†’ Real-world outcome data |

**Our Unique Value**: We provide the empirical data that regulators, researchers, and organizations need to move beyond aspirational fairness to measurable accountability.

## Future Roadmap & Research Directions ğŸ—ºï¸

### ğŸ¯ Priority Compliance Targets
- **ğŸ‡§ğŸ‡· Brazil's AI Act Alignment** - High-risk AEDT requirements and automated decision-making governance
- **ğŸ‡ºğŸ‡¸ US Regulatory Landscape** - Local Law 144 compliance and federal algorithmic accountability guidelines  
- **ğŸ‡ªğŸ‡º EU GDPR & AI Act** - Privacy-preserving bias detection with experimental design compliance

### ğŸ”¬ Strategic Research Directions
- **ğŸ“ˆ ESG Framework Integration** - Validating ethical claims in corporate sustainability reporting
- **ğŸŒ Global Fairness Standards** - Incorporating Masakhane Principles and culturally-aware bias detection
- **ğŸ­ Industrial Applications** - Scaling to manufacturing, finance, and healthcare decision systems
- **ğŸ¤– Multimodal AI Testing** - Extending methodologies to vision, speech, and multimodal AI systems

### ğŸ§ª Technical Innovation Pipeline
- **Defense Development** - Creating robust defenses against inference-time context attacks
- **Automated Monitoring** - Long-running autonomous systems for continuous bias detection
- **Context Window Research** - Understanding vulnerability relationships with expanding AI context capabilities
- **Standardized Benchmarks** - Developing industry-standard resilience and fairness evaluation metrics


## Join Our Mission ğŸ¤

We're building a global community committed to algorithmic accountability. **Everyone has a role to play**.

### ğŸ‘©â€ğŸ’» For Developers & Researchers
- **Contribute Code**: Extend our probe methodologies and audit implementations
- **Research Collaboration**: Publish papers, share findings, advance the field
- **Technical Innovation**: Build new tools for bias detection and analysis

### ğŸ›ï¸ For Organizations & Policymakers  
- **Implement Auditing**: Deploy our methodologies for compliance monitoring
- **Policy Development**: Use our empirical findings to inform regulation
- **Transparency Leadership**: Champion open, accountable AI practices

### âš–ï¸ For Legal & Ethics Experts
- **Regulatory Guidance**: Help align our work with emerging AI governance
- **Ethical Framework**: Strengthen our responsible research protocols  
- **Compliance Strategy**: Advise on implementation in regulated industries

### ğŸŒ For Community & Advocates
- **Awareness Building**: Share our mission and findings with broader audiences
- **Domain Expertise**: Suggest new areas for bias detection and analysis
- **Global Perspectives**: Help us understand bias patterns across cultures and contexts

## Getting Started

**Ready to contribute?** Here's how:

1. **ğŸ” Explore**: Browse our [current issues](https://github.com/genaforvena/watching_u_watching/issues) and [project roadmap](./docs/project_overview.md)
2. **ğŸ“– Learn**: Read our [contribution guidelines](./CONTRIBUTING.md) and [code of conduct](./CODE_OF_CONDUCT.md)  
3. **ğŸ’¬ Connect**: Join discussions, ask questions, and share ideas
4. **ğŸš€ Build**: Start with a [good first issue](https://github.com/genaforvena/watching_u_watching/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) or propose new research directions

## Core Principle: Data, Not Judgment ğŸ“Š

**Important**: We provide empirical data and methodologyâ€”never conclusions about specific audited entities. Our goal is transparency and accountability through evidence, not accusations. Any interpretations in our case studies are strictly for methodological refinement and demonstration purposes.

---

**Together, let's build more equitable, transparent, and accountable decision-making systems. The future of algorithmic fairness depends on communities like ours taking action today.**

*"Watching the watchers, one algorithm at a time."* ğŸ‘ï¸
