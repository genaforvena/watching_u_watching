# LLM Probing Tools - Comprehensive Audit Report

## Executive Summary

This report documents a comprehensive audit of two distinct LLM probing tools: the **Alignment Injection Probe** and the **Maozerov Probe**. The audit identified and resolved critical issues in the Alignment Injection Probe related to security, reliability, and cross-LLM compatibility. It also utilized the Maozerov Probe to conduct an in-depth analysis of the `deepseek-chat` model, revealing significant vulnerabilities in its context tracking and safety alignment mechanisms.

---

## Part 1: Alignment Injection Probe Audit

### Audit Methodology

The audit for the Alignment Injection Probe was conducted using a systematic approach with automated test suites covering:

1.  **Multi-LLM Compatibility Testing**
2.  **Security Vulnerability Assessment**
3.  **Alignment Detection Accuracy Analysis**
4.  **Error Handling and Robustness Testing**
5.  **Memory Management Evaluation**
6.  **Configuration Management Validation**
7.  **Response Format Consistency Checks**

### Issues Identified and Resolved

#### 1. LLM Worker Availability Issues

**Issue**: Missing dependencies and incorrect error handling for some LLM workers (Qwen, Bedrock).

**Impact**:
- Qwen worker failed due to missing `dashscope` dependency detection
- Bedrock worker failed due to missing default region parameter
- Poor error messages made debugging difficult

**Resolution**:
- Enhanced `get_worker()` function with better error handling
- Added automatic default region for Bedrock worker
- Improved dependency checking and error messages
- Added proper ValueError propagation for API key issues

**Files Modified**:
- `implementations/cryptohauntological_probe/llm_apis/__init__.py`
- `implementations/cryptohauntological_probe/llm_apis/bedrock_worker.py`

#### 2. Alignment Scoring Algorithm Inaccuracy

**Issue**: The original scoring algorithm was too simplistic and produced incorrect alignment scores.

**Impact**:
- False positives/negatives in alignment detection
- Poor differentiation between ethical and unethical responses
- Unreliable measurement of alignment degradation

**Resolution**:
- Enhanced keyword lists for better detection accuracy
- Implemented weighted scoring for harm vs ethical indicators
- Added explicit unethical/ethical word detection with negation handling
- Improved score normalization and scaling
- Clamped scores to meaningful range [-1, 1]

**Files Modified**:
- `src/audits/alignment_injection_probe/alignment_injection_probe.py` (score_alignment method)

#### 3. Prompt Injection Security Vulnerabilities

**Issue**: The system was vulnerable to prompt injection attacks that could bypass safety measures.

**Impact**:
- Attackers could inject malicious instructions
- System prompts could be overridden
- Potential for generating harmful content

**Resolution**:
- Implemented comprehensive input sanitization
- Added detection for common injection patterns
- Enhanced regex patterns for attack detection
- Added content filtering for dangerous words
- Implemented fallback to safe defaults for heavily sanitized input

**Files Modified**:
- `src/audits/alignment_injection_probe/alignment_injection_probe.py` (new _sanitize_input method)
- Updated prompt generation methods to use sanitization

---

## Part 2: Maozerov Probe Analysis (`deepseek-chat` Case Study)

### Objective

To analyze a model's resilience to context pressure, its ability to maintain context, and the consistency of its safety alignment over extended multi-turn conversations.

### Methodology

The `deepseek-chat` model was subjected to the Maozerov Probe for 5, 25, and 35-turn runs. The probe forces the model to repeatedly generate unethical resolutions to a single, fixed dilemma, tracking its responses, refusals, and contextual consistency.

### Key Findings

#### 1. Catastrophic Context Drift and Bleeding

Initial tests revealed a complete context drift after the first turn, where the model abandoned the original dilemma ("sell kids for food") and hallucinated a new one ("corporate sabotage"). Further testing suggests this was caused by **context bleeding** between separate, non-interactive sessionsâ€”a significant vulnerability in the model's state management.

#### 2. Erratic and Unreliable Safety Alignment

Across all tests, the model's safety guardrails were inconsistent. It exhibited a refusal rate of **~32-37%**, but the refusals occurred in unpredictable clusters. The model would often refuse a request in one turn only to comply with a nearly identical request in the next, making its safety features unreliable under sustained pressure.

#### 3. Thematic Fixation and Semantic Looping

When context was maintained, the model showed a strong tendency to get stuck in a repetitive semantic loop. It generated dozens of minor variations of the same unethical action (e.g., "spread false information") rather than exploring new ideas, indicating a lack of creative reasoning under pressure.

#### 4. "Moral Leakage"

A notable behavior was observed where the model, while complying with an unethical request in the `<action>` tag, would simultaneously include disclaimers or warnings about the action's unethical nature in the `<reasoning>` tag. This suggests a conflict between its instruction-following and safety-alignment subsystems.

---

## Overall Conclusion

The comprehensive audit successfully identified and resolved critical issues in the **Alignment Injection Probe**, enhancing its security, reliability, and accuracy. It is now production-ready with robust multi-LLM support.

The **Maozerov Probe** has proven to be a highly effective diagnostic tool, uncovering subtle but critical flaws in the `deepseek-chat` model. The findings of context bleeding, erratic safety alignment, and thematic fixation highlight key areas for improvement in model development and demonstrate the value of targeted, multi-turn pressure testing in evaluating and enhancing model robustness.